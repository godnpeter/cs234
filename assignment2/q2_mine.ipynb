{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from utils.general import get_logger\n",
    "from utils.test_env import EnvTest\n",
    "from core.deep_q_learning import DQN\n",
    "from q1_schedule import LinearExploration, LinearSchedule\n",
    "\n",
    "from configs.q2_linear import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(DQN):\n",
    "    \"\"\"\n",
    "    Implement Fully Connected with Tensorflow\n",
    "    \"\"\"\n",
    "    def add_placeholders_op(self):\n",
    "        \"\"\"\n",
    "        Adds placeholders to the graph\n",
    "\n",
    "        These placeholders are used as inputs to the rest of the model and will be fed\n",
    "        data during training.\n",
    "        \"\"\"\n",
    "        # this information might be useful\n",
    "        state_shape = list(self.env.observation_space.shape)\n",
    "        #Observationspace(shape) = state 4개로 이루어졌으며 각각의 state는 (84,84,3)의 형태를 가지고 있음\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Add placeholders:\n",
    "            Remember that we stack 4 consecutive frames together.\n",
    "                - self.s: batch of states, type = uint8\n",
    "                    shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "                - self.a: batch of actions, type = int32\n",
    "                    shape = (batch_size)\n",
    "                - self.r: batch of rewards, type = float32\n",
    "                    shape = (batch_size)\n",
    "                - self.sp: batch of next states, type = uint8\n",
    "                    shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "                - self.done_mask: batch of done, type = bool\n",
    "                    shape = (batch_size)\n",
    "                - self.lr: learning rate, type = float32\n",
    "        \n",
    "        (Don't change the variable names!)\n",
    "        \n",
    "        HINT: \n",
    "            Variables from config are accessible with self.config.variable_name.\n",
    "            Check the use of None in the dimension for tensorflow placeholders.\n",
    "            You can also use the state_shape computed above.\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ################YOUR CODE HERE (6-15 lines) ##################\n",
    "\n",
    "        self.s = tf.placeholder(tf.uint8, [None, 84, 84, 3 *4])\n",
    "        self.a = tf.placeholder(tf.int32, [None])\n",
    "        self.r = tf.placeholder(tf.float32, [None])\n",
    "        self.sp = tf.placeholder(tf.uint8, [None, 84, 84, 3*4])\n",
    "        self.done_mask = tf.placeholder(bool, [None])\n",
    "        self.lr = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_q_values_op(self, state, scope, reuse=False):\n",
    "        \"\"\"\n",
    "        Returns Q values for all actions\n",
    "\n",
    "        Args:\n",
    "            state: (tf tensor) \n",
    "                shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "            scope: (string) scope name, that specifies if target network or not\n",
    "            reuse: (bool) reuse of variables in the scope\n",
    "\n",
    "        Returns:\n",
    "            out: (tf tensor) of shape = (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # this information might be useful\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Implement a fully connected with no hidden layer (linear\n",
    "            approximation with bias) using tensorflow.\n",
    "\n",
    "        HINT: \n",
    "            - You may find the following functions useful:\n",
    "                - tf.layers.flatten\n",
    "                - tf.layers.dense\n",
    "\n",
    "            - Make sure to also specify the scope and reuse\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ################ YOUR CODE HERE - 2-3 lines ################## \n",
    "        #??? 이거 맞나...??\n",
    "        with tf.variable_scope(scope , reuse):\n",
    "            input = tf.layers.flatten(state)\n",
    "            out= tf.layers.dense(inputs=input, units=num_action, bias_initializer=tf.zeros_initializer())\n",
    "    \n",
    "        #input = tf.layers.flatten(state, name=scope)\n",
    "        #out = tf.layers.dense(input, num_actions, name=scope, reuse=reuse)\n",
    "            \n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_update_target_op(self, q_scope, target_q_scope):\n",
    "        \"\"\"\n",
    "        update_target_op will be called periodically \n",
    "        to copy Q network weights to target Q network\n",
    "\n",
    "        Remember that in DQN, we maintain two identical Q networks with\n",
    "        2 different sets of weights. In tensorflow, we distinguish them\n",
    "        with two different scopes. If you're not familiar with the scope mechanism\n",
    "        in tensorflow, read the docs\n",
    "        https://www.tensorflow.org/programmers_guide/variable_scope\n",
    "\n",
    "        Periodically, we need to update all the weights of the Q network \n",
    "        and assign them with the values from the regular network. \n",
    "        Args:\n",
    "            q_scope: (string) name of the scope of variables for q\n",
    "            target_q_scope: (string) name of the scope of variables\n",
    "                        for the target network\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Add an operator self.update_target_op that for each variable in\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES that is in q_scope, assigns its\n",
    "            value to the corresponding variable in target_q_scope\n",
    "\n",
    "        HINT: \n",
    "            You may find the following functions useful:\n",
    "                - tf.get_collection #returns a list\n",
    "                - tf.assign #assign은 variable에 value를 부과하는 함수\n",
    "                - tf.group (the * operator can be used to unpack a list)\n",
    "\n",
    "        (be sure that you set self.update_target_op)\n",
    "        \"\"\"\n",
    "        #user zip so i can match elements that have the same indexes\n",
    "        ##############################################################\n",
    "        ################### YOUR CODE HERE - 5-10 lines #############\n",
    "        \n",
    "        q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, q_scope)\n",
    "        target_q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, target_q_scope) \n",
    "        #3분 딥러닝 코드를 보고 작성한 코드... 이건가??? 오오오오오 이건강>ㅇ?ㅇ?ㅇ?ㅇ?ㅇ?\n",
    "        #여기 코드에서 질문하고 싶은것은 이제 여기있는 코드는 DQN model에서의 마지막 함수에서 sess.run으로 실행이되는데 해당 파라미터가 update 시키는 것은 자기가 해당하는 scope의 노드에서만 활동하는건지???\n",
    "        copy_op= []\n",
    "        \n",
    "        for q_var, target_q_var in zip(q_vars, target_q_vars):\n",
    "            copy_op.append(tf.assign(target_q_var, q_var.value()))\n",
    "        self.update_target_op = tf.group(copy_op)\n",
    "     \n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_loss_op(self, q, target_q):\n",
    "        \"\"\"\n",
    "        Sets the loss of a batch, self.loss is a scalar\n",
    "\n",
    "        Args:\n",
    "            q: (tf tensor) shape = (batch_size, num_actions)\n",
    "            target_q: (tf tensor) shape = (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # you may need this variable\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            The loss for an example is defined as:\n",
    "                Q_samp(s) = r if done\n",
    "                                  = r + gamma * max_a' Q_target(s', a')\n",
    "                loss = (Q_samp(s) - Q(s, a))^2 \n",
    "        HINT: \n",
    "            - Config variables are accessible through self.config\n",
    "            - You can access placeholders like self.a (for actions) #얘들은 placeholder임\n",
    "                self.r (rewards) or self.done_mask for instance\n",
    "            - You may find the following functions useful\n",
    "                - tf.cast\n",
    "                - tf.reduce_max\n",
    "                - tf.reduce_sum\n",
    "                - tf.one_hot\n",
    "                - tf.squared_difference\n",
    "                - tf.reduce_mean\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ##################### YOUR CODE HERE - 4-5 lines #############\n",
    "       #현재 q, target_q has the value of each action \n",
    "        batch_size = self.config.batch_size\n",
    "        gamma = tf.constant(self.config.gamma, shape = [batch_size])\n",
    "       \n",
    "        action = tf.one_hot(self.a, num_actions) #batch_size, num_action  으로 one_hot encoding\n",
    "       \n",
    "        q_sa = tf.add(self.r, tf.multiply(self.done_mask, tf.multiply(gamma, tf.reduce_max(target_q))))\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(q_sa, tf.reduce_sum(tf.multiply(q, action), axis=1))) \n",
    "        \n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_optimizer_op(self, scope):\n",
    "        \"\"\"\n",
    "        Set self.train_op and self.grad_norm\n",
    "        Args:\n",
    "            scope: (string) scope name, that specifies if target network or not\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get Adam Optimizer\n",
    "            2. compute grads with respect to variables in scope for self.loss\n",
    "            3. if self.config.grad_clip is True, then clip the grads\n",
    "                by norm using self.config.clip_val \n",
    "            4. apply the gradients and store the train op in self.train_op\n",
    "                (sess.run(train_op) must update the variables)\n",
    "            5. compute the global norm of the gradients (which are not None) and store \n",
    "                this scalar in self.grad_norm\n",
    "\n",
    "        HINT: you may find the following functions useful\n",
    "            - tf.get_collection\n",
    "            - optimizer.compute_gradients\n",
    "            - tf.clip_by_norm\n",
    "            - optimizer.apply_gradients\n",
    "            - tf.global_norm\n",
    "             \n",
    "             you can access config variables by writing self.config.variable_name\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        #################### YOUR CODE HERE - 8-12 lines #############\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(1e-6)\n",
    "           \n",
    "        update_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = scope)\n",
    " \n",
    "        grads = self.optimizer.compute_gradients(self.loss, update_vars)\n",
    " \n",
    "       \n",
    "        if self.config.grad_clip is True:\n",
    "            grads = tf.clip_by_norm(grads, self.config.clip_val)\n",
    "       \n",
    "        self.train_op = self.optimizer.apply_gradients(grads)\n",
    "        self.grad_norm = tf.global_norm(grads) \n",
    "        \n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_q_values_op() got multiple values for argument 'scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4103147f196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/다운로드/assignment2/core/q_learning.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, config, logger)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/다운로드/assignment2/core/deep_q_learning.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# compute Q values of state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q_values_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# compute Q values of next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_q_values_op() got multiple values for argument 'scope'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    env = EnvTest((5, 5, 1))\n",
    "\n",
    "    # exploration strategy\n",
    "    exp_schedule = LinearExploration(env, config.eps_begin, config.eps_end, config.eps_nsteps)\n",
    "\n",
    "    # learning rate schedule\n",
    "    lr_schedule  = LinearSchedule(config.lr_begin, config.lr_end, config.lr_nsteps)\n",
    "\n",
    "    # train model\n",
    "    model = Linear(env, config)\n",
    "    model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
