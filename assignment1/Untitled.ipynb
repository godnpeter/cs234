{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_n=(0.2,0.31,0.19,0.3)\n",
    "\n",
    "prob_n = np.asarray(prob_n)\n",
    "csprob_n = np.cumsum(prob_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2 , 0.51, 0.7 , 1.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csprob_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b003571ae7b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np_random' is not defined"
     ]
    }
   ],
   "source": [
    "(csprob_n > np_random.rand()).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict= {}\n",
    "mydict[('po', 'lo')]= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('po', 'lo'): 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict['po', 'lo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po = np.array((1,2,3,6,5))\n",
    "po.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "if 3 is not po.argmax():\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -4,  0,  1,  3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do = np.array((2,6,3,5,2))\n",
    "po-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdiff = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxdiff>100000000000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 6 5] [  2   6 100   5   2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-95c70c807059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "print(po, do)\n",
    "po = np.array((1,2,3,6,5))\n",
    "do = np.array((2,6,100,5,2))\n",
    "max(do, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nA =4\n",
    "action_value =[0 for a in range(nA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(abs(po-do))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from itertools import product\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "    P: nested dictionary\n",
    "        From gym.core.Environment\n",
    "        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "        tuple of the form (probability, nextstate, reward, terminal) where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS: int\n",
    "        number of states in the environment\n",
    "    nA: int\n",
    "        number of actions in the environment\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "\"\"\"\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(nS)\n",
    "\n",
    "    max_diff = float('inf')\n",
    "\n",
    "    while max_diff > tol:\n",
    "        next_value_function = value_function.copy()\n",
    "        max_diff = -float('inf')\n",
    "        \n",
    "        for state in range(nS):\n",
    "            policy_action = policy[state]\n",
    "            expected_return = 0\n",
    "            for  probability, nextstate, reward, terminal in P[state][policy_action]:\n",
    "                expected_return += probability * (reward + gamma * value_function[nextstate])\n",
    "\n",
    "            next_value_function[state] = expected_return\n",
    "\n",
    "            max_diff = max(max_diff, abs(value_function[state] - next_value_function[state]))\n",
    "\n",
    "        value_function = next_value_function\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "    for state in range(nS):\n",
    "        max_val = argmax = -float('inf')\n",
    "\n",
    "        for action in range(nA):\n",
    "            result = 0\n",
    "            for prob, nextstate, reward, _ in P[state][action]:\n",
    "                result += prob * (reward + gamma * value_from_policy[nextstate])\n",
    "\n",
    "            if result > max_val:\n",
    "                max_val, argmax = result, action\n",
    "\n",
    "        new_policy[state] = argmax\n",
    "\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        tol parameter used in policy_evaluation()\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "    policy = np.zeros(nS)\n",
    "    value_function = np.zeros(nS)\n",
    "    maxdiff = float('inf')\n",
    "    num_iters = 0\n",
    "\n",
    "    while maxdiff > tol or num_iters < 3:\n",
    "        next_value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        policy = policy_improvement(P, nS, nA, next_value_function, policy, gamma)\n",
    "        maxdiff = max(abs(next_value_function - value_function))\n",
    "        value_function = next_value_function\n",
    "        num_iters += 1\n",
    "\n",
    "    print(value_function)\n",
    "    print(f'policy iteration took {num_iters} to converge')\n",
    "    return value_function, policy\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    \n",
    "    maxdiff = float('inf')\n",
    "    num_iters = 0 \n",
    "    while maxdiff > tol:\n",
    "        num_iters += 1\n",
    "        next_value_function = value_function.copy()\n",
    "        maxdiff = -float('inf')\n",
    "\n",
    "        for state in range(nS):\n",
    "            best_return = -float('inf')\n",
    "\n",
    "            for action in range(nA):\n",
    "                exp_return = 0\n",
    "                for prob, nextstate, reward, _ in P[state][action]:\n",
    "                    exp_return += prob * (reward + gamma * value_function[nextstate])\n",
    "\n",
    "                best_return = max(best_return, exp_return)\n",
    "\n",
    "            next_value_function[state] = best_return\n",
    "            maxdiff = max(maxdiff, abs(next_value_function[state] - value_function[state]))\n",
    "\n",
    "        value_function = next_value_function\n",
    "\n",
    "    print(f'value iteration took {num_iters} iterations to converge')\n",
    "    return value_function, policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "\n",
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "      print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    #env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_pi, 100)\n",
    "\n",
    "    #print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    #V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    #render_single(env, p_vi, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
